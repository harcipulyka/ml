{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "Main takeaways:\n",
    " - In this case Z-Score provided a real advantage, resulting in the best model so far\n",
    " - Higher learning rate is preferred with lower feature values\n",
    " - MinMax didn't provide an edge over the basic dataset\n",
    " \n",
    " Known limitations of this notebook:\n",
    " - no test-set-validation\n",
    " - no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from helper import charts, lib\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import the dataset.\n",
    "training_df = pd.read_csv(filepath_or_buffer=\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "\n",
    "#scale the label\n",
    "training_df[\"median_house_value\"] /= 1000.0\n",
    "\n",
    "#set the label name early\n",
    "my_label=\"median_house_value\" # the median value of a house on a specific city block."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outlying values, creating 4 versions: \n",
    " - outliers removed from both housing_median_age and median_house_value\n",
    " - outliers removed from only one of them\n",
    " - no outliers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 1052 elements in the ' housing_median_age ' column with the value 52.0 (max value)\n",
      "Dataset contains 814 elements in the ' median_house_value ' column with the value 500.001 (max value)\n"
     ]
    }
   ],
   "source": [
    "#names will be hma, mhv, both, none\n",
    "\n",
    "#because they occur a lot\n",
    "hma = \"housing_median_age\"\n",
    "mhv = \"median_house_value\"\n",
    "\n",
    "hma_max = training_df[hma].max()\n",
    "_hma_drop = training_df.loc[training_df[hma] == hma_max].index\n",
    "print(\"Dataset contains\", len(_hma_drop), \"elements in the '\",hma,\"' column with the value\", hma_max, \"(max value)\")\n",
    "\n",
    "mhv_max = training_df[mhv].max()\n",
    "_mhv_drop = training_df.loc[training_df[mhv] == mhv_max].index\n",
    "print(\"Dataset contains\", len(_mhv_drop), \"elements in the '\",mhv,\"' column with the value\", mhv_max, \"(max value)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hma_df = training_df.drop(_hma_drop)\n",
    "mhv_df = training_df.drop(_mhv_drop)\n",
    "both_df = hma_df.drop(_mhv_drop, errors='ignore')\n",
    "none_df = training_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a simple linear regression model.\"\"\"\n",
    "  # Most simple tf.keras models are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(units=1, input_shape=(6,)))\n",
    " \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  return model        \n",
    "\n",
    "\n",
    "def train_model(model, training_df, feature, label, epochs, batch_size):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "  history = model.fit(\n",
    "    x=training_df[feature],\n",
    "    y=training_df[label],\n",
    "    batch_size=batch_size,\n",
    "    #validation_data=(training_df[feature], training_df[label]), #the weights shouldn't be influenced by the model, but the overall accurarcy should be measured against all datapoints\n",
    "    epochs=epochs)\n",
    "\n",
    "  return pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the feature and the label.\n",
    "my_feature = [\"longitude\", \"latitude\", \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Epoch 1/20\n",
      "266/266 [==============================] - 3s 11ms/step - loss: 177553.6875 - root_mean_squared_error: 421.3712 - val_loss: 78242.4375 - val_root_mean_squared_error: 279.7185\n",
      "Epoch 2/20\n",
      "266/266 [==============================] - 2s 9ms/step - loss: 45846.2344 - root_mean_squared_error: 214.1173 - val_loss: 28870.8379 - val_root_mean_squared_error: 169.9142\n",
      "Epoch 3/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 22083.2949 - root_mean_squared_error: 148.6045 - val_loss: 16413.1055 - val_root_mean_squared_error: 128.1136\n",
      "Epoch 4/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 14382.7139 - root_mean_squared_error: 119.9280 - val_loss: 12976.9707 - val_root_mean_squared_error: 113.9165\n",
      "Epoch 5/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12501.8184 - root_mean_squared_error: 111.8115 - val_loss: 12229.6191 - val_root_mean_squared_error: 110.5876\n",
      "Epoch 6/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12199.9131 - root_mean_squared_error: 110.4532 - val_loss: 12168.6523 - val_root_mean_squared_error: 110.3116\n",
      "Epoch 7/20\n",
      "266/266 [==============================] - 2s 9ms/step - loss: 12082.5742 - root_mean_squared_error: 109.9208 - val_loss: 12210.0684 - val_root_mean_squared_error: 110.4992\n",
      "Epoch 8/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12153.1846 - root_mean_squared_error: 110.2415 - val_loss: 11975.2139 - val_root_mean_squared_error: 109.4313\n",
      "Epoch 9/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12142.0234 - root_mean_squared_error: 110.1908 - val_loss: 11980.2900 - val_root_mean_squared_error: 109.4545\n",
      "Epoch 10/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12121.0400 - root_mean_squared_error: 110.0956 - val_loss: 11881.7305 - val_root_mean_squared_error: 109.0034\n",
      "Epoch 11/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12045.2891 - root_mean_squared_error: 109.7510 - val_loss: 12969.5645 - val_root_mean_squared_error: 113.8840\n",
      "Epoch 12/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12225.8623 - root_mean_squared_error: 110.5706 - val_loss: 12902.0186 - val_root_mean_squared_error: 113.5871\n",
      "Epoch 13/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12179.2373 - root_mean_squared_error: 110.3596 - val_loss: 12466.6299 - val_root_mean_squared_error: 111.6541\n",
      "Epoch 14/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12316.9658 - root_mean_squared_error: 110.9818 - val_loss: 13016.2773 - val_root_mean_squared_error: 114.0889\n",
      "Epoch 15/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12391.8125 - root_mean_squared_error: 111.3185 - val_loss: 11843.4785 - val_root_mean_squared_error: 108.8278\n",
      "Epoch 16/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12177.6729 - root_mean_squared_error: 110.3525 - val_loss: 11946.5771 - val_root_mean_squared_error: 109.3004\n",
      "Epoch 17/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12317.6250 - root_mean_squared_error: 110.9848 - val_loss: 11964.0645 - val_root_mean_squared_error: 109.3804\n",
      "Epoch 18/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12196.5557 - root_mean_squared_error: 110.4380 - val_loss: 11749.7100 - val_root_mean_squared_error: 108.3961\n",
      "Epoch 19/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 12334.4971 - root_mean_squared_error: 111.0608 - val_loss: 11996.1650 - val_root_mean_squared_error: 109.5270\n",
      "Epoch 20/20\n",
      "266/266 [==============================] - 2s 8ms/step - loss: 11944.5205 - root_mean_squared_error: 109.2910 - val_loss: 11956.7129 - val_root_mean_squared_error: 109.3468\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for none\n",
    "learning_rate = 0.01\n",
    "epoch_count = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Build model\n",
    "none_model = build_model(learning_rate)\n",
    "none_history = train_model(none_model, none_df, my_feature, my_label, epoch_count, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "253/253 [==============================] - 2s 9ms/step - loss: 474432.2812 - root_mean_squared_error: 688.7905 - val_loss: 20219.5918 - val_root_mean_squared_error: 142.1956\n",
      "Epoch 2/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 17157.9414 - root_mean_squared_error: 130.9883 - val_loss: 14169.0605 - val_root_mean_squared_error: 119.0339\n",
      "Epoch 3/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 12526.7012 - root_mean_squared_error: 111.9227 - val_loss: 11092.9180 - val_root_mean_squared_error: 105.3229\n",
      "Epoch 4/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 10413.9014 - root_mean_squared_error: 102.0485 - val_loss: 9753.4297 - val_root_mean_squared_error: 98.7595\n",
      "Epoch 5/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 9505.0469 - root_mean_squared_error: 97.4938 - val_loss: 9196.6914 - val_root_mean_squared_error: 95.8994\n",
      "Epoch 6/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 9072.0352 - root_mean_squared_error: 95.2472 - val_loss: 8872.3037 - val_root_mean_squared_error: 94.1929\n",
      "Epoch 7/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8882.1338 - root_mean_squared_error: 94.2451 - val_loss: 8754.1240 - val_root_mean_squared_error: 93.5635\n",
      "Epoch 8/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8825.4033 - root_mean_squared_error: 93.9436 - val_loss: 8735.5176 - val_root_mean_squared_error: 93.4640\n",
      "Epoch 9/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8791.0566 - root_mean_squared_error: 93.7606 - val_loss: 8691.4443 - val_root_mean_squared_error: 93.2279\n",
      "Epoch 10/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8778.4727 - root_mean_squared_error: 93.6935 - val_loss: 8769.9580 - val_root_mean_squared_error: 93.6480\n",
      "Epoch 11/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8737.6104 - root_mean_squared_error: 93.4752 - val_loss: 8789.1084 - val_root_mean_squared_error: 93.7502\n",
      "Epoch 12/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8728.6846 - root_mean_squared_error: 93.4274 - val_loss: 8723.8535 - val_root_mean_squared_error: 93.4016\n",
      "Epoch 13/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8842.7881 - root_mean_squared_error: 94.0361 - val_loss: 8651.3330 - val_root_mean_squared_error: 93.0125\n",
      "Epoch 14/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8761.7969 - root_mean_squared_error: 93.6045 - val_loss: 8674.5195 - val_root_mean_squared_error: 93.1371\n",
      "Epoch 15/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8789.0469 - root_mean_squared_error: 93.7499 - val_loss: 8722.8652 - val_root_mean_squared_error: 93.3963\n",
      "Epoch 16/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8822.0752 - root_mean_squared_error: 93.9259 - val_loss: 8737.4609 - val_root_mean_squared_error: 93.4744\n",
      "Epoch 17/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8755.5176 - root_mean_squared_error: 93.5709 - val_loss: 8683.1152 - val_root_mean_squared_error: 93.1832\n",
      "Epoch 18/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8931.1992 - root_mean_squared_error: 94.5050 - val_loss: 8671.2637 - val_root_mean_squared_error: 93.1196\n",
      "Epoch 19/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 8812.3018 - root_mean_squared_error: 93.8739 - val_loss: 9224.9502 - val_root_mean_squared_error: 96.0466\n",
      "Epoch 20/20\n",
      "253/253 [==============================] - 2s 8ms/step - loss: 9351.6387 - root_mean_squared_error: 96.7039 - val_loss: 9044.0088 - val_root_mean_squared_error: 95.1000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for mhv\n",
    "learning_rate = 0.01\n",
    "epoch_count = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Build model\n",
    "mhv_model = build_model(learning_rate)\n",
    "mhv_history = train_model(mhv_model, mhv_df, my_feature, my_label, epoch_count, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 83857.7344 - root_mean_squared_error: 289.5820 - val_loss: 15571.4951 - val_root_mean_squared_error: 124.7858\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 12402.3535 - root_mean_squared_error: 111.3659 - val_loss: 11321.9463 - val_root_mean_squared_error: 106.4046\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11461.1719 - root_mean_squared_error: 107.0569 - val_loss: 11257.7461 - val_root_mean_squared_error: 106.1025\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11399.2686 - root_mean_squared_error: 106.7674 - val_loss: 11239.9521 - val_root_mean_squared_error: 106.0186\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11511.9902 - root_mean_squared_error: 107.2939 - val_loss: 11215.9219 - val_root_mean_squared_error: 105.9053\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11619.6045 - root_mean_squared_error: 107.7943 - val_loss: 11248.0498 - val_root_mean_squared_error: 106.0568\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11488.6074 - root_mean_squared_error: 107.1849 - val_loss: 12586.4922 - val_root_mean_squared_error: 112.1895\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 11770.7227 - root_mean_squared_error: 108.4930 - val_loss: 11790.4277 - val_root_mean_squared_error: 108.5837\n",
      "Epoch 9/20\n",
      "249/250 [============================>.] - ETA: 0s - loss: 11763.1738 - root_mean_squared_error: 108.4582"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Build model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m hma_model \u001b[39m=\u001b[39m build_model(learning_rate)\n\u001b[0;32m----> 8\u001b[0m hma_history \u001b[39m=\u001b[39m train_model(hma_model, hma_df, my_feature, my_label, epoch_count, batch_size)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, training_df, feature, label, epochs, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, training_df, feature, label, epochs, batch_size):\n\u001b[1;32m     15\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Train the model by feeding it data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m   history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     17\u001b[0m     x\u001b[39m=\u001b[39;49mtraining_df[feature],\n\u001b[1;32m     18\u001b[0m     y\u001b[39m=\u001b[39;49mtraining_df[label],\n\u001b[1;32m     19\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     20\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(training_df[feature], training_df[label]), \u001b[39m##important, validation is done on all the values, not just hte cleaned up ones\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs)\n\u001b[1;32m     23\u001b[0m   \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(history\u001b[39m.\u001b[39mhistory)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[1;32m   1593\u001b[0m         x\u001b[39m=\u001b[39mval_x,\n\u001b[1;32m   1594\u001b[0m         y\u001b[39m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution,\n\u001b[1;32m   1605\u001b[0m     )\n\u001b[0;32m-> 1606\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m   1607\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m   1608\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m   1609\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[1;32m   1610\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[1;32m   1611\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m   1612\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1613\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1614\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1615\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1616\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1617\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1618\u001b[0m )\n\u001b[1;32m   1619\u001b[0m val_logs \u001b[39m=\u001b[39m {\n\u001b[1;32m   1620\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()\n\u001b[1;32m   1621\u001b[0m }\n\u001b[1;32m   1622\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/keras/engine/training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1943\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1944\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m   1945\u001b[0m ):\n\u001b[1;32m   1946\u001b[0m     callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m-> 1947\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[1;32m   1948\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1949\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateful_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters for HMA\n",
    "learning_rate = 0.01\n",
    "epoch_count = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Build model\n",
    "hma_model = build_model(learning_rate)\n",
    "hma_history = train_model(hma_model, hma_df, my_feature, my_label, epoch_count, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hyperparameters for both\n",
    "learning_rate = 0.01\n",
    "epoch_count = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Build model\n",
    "both_model = build_model(learning_rate)\n",
    "both_history = train_model(both_model, both_df, my_feature, my_label, epoch_count, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#examining history\n",
    "rmse = \"root_mean_squared_error\"\n",
    "\n",
    "none_key = \"Original\"\n",
    "mhv_key = \"Mean House Value cleaned up\"\n",
    "hma_key = \"Housing Median Age cleaned up\"\n",
    "both_key = \"Both cleaned up\"\n",
    "\n",
    "keys = [none_key, mhv_key, hma_key, both_key]\n",
    "\n",
    "together_df = pd.DataFrame({\n",
    "    none_key : none_history[rmse],\n",
    "    mhv_key : mhv_history[rmse],\n",
    "    hma_key : hma_history[rmse],\n",
    "    both_key : both_history[rmse]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#plotting basic rmse history of the three different methods\n",
    "charts.plot_training_losses(losses=keys, df=together_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"No cleaning metrics:\")\n",
    "_ = none_model.evaluate(training_df[my_feature], training_df[my_label])\n",
    "\n",
    "print(\"MHV cleaned metrics:\")\n",
    "_ = mhv_model.evaluate(training_df[my_feature], training_df[my_label])\n",
    "\n",
    "print(\"HMA cleaned metrics:\")\n",
    "_ = hma_model.evaluate(training_df[my_feature], training_df[my_label])\n",
    "\n",
    "print(\"Both cleaned metrics:\")\n",
    "_ = both_model.evaluate(training_df[my_feature], training_df[my_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#sampling data\n",
    "sample_normal = lib.create_inference_sample(training_df, my_feature, basic_model)\n",
    "sample_z = lib.create_inference_sample(z_df, my_feature, z_model)\n",
    "sample_minmax = lib.create_inference_sample(minmax_df, my_feature, minmax_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "charts.plot_training_test_validat_accuracy(\n",
    "    dfs=[sample_normal, sample_z, sample_minmax],\n",
    "    label_key=my_label,\n",
    "    titles=keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.9.6 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#weights per features\n",
    "weights = pd.DataFrame.from_dict({\n",
    "    \"feature\" : my_feature,\n",
    "    basic_key : basic_model.get_weights()[0].ravel(),\n",
    "    z_key : z_model.get_weights()[0].ravel(),\n",
    "    minmax_key : minmax_model.get_weights()[0].ravel(),\n",
    "})\n",
    "\n",
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
